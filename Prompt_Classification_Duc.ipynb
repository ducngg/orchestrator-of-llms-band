{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3IL6wVs0nbQ"
      },
      "source": [
        "# Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B05r84sH0uAD"
      },
      "source": [
        "## Prompt template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "h6SWF-iqkG0Y"
      },
      "outputs": [],
      "source": [
        "from prompt import format_prompt, SCENARIOS_INFO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_CDNzV00y_D"
      },
      "source": [
        "## Result parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "vO-I_C32xWPQ"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "SCENARIOS = set([\n",
        "    'Alignment', 'Quality', 'Bias', 'Fidelity', 'Question-answering', 'Information retrieval',\n",
        "    'Summarization', 'Sentiment analysis', 'Toxicity detection', 'Text classification', 'Language', 'Knowledge',\n",
        "    'Reasoning', 'Harms', 'Vary number of in-context examples', 'Vary multiple-choice strategy', 'Vary prompting', 'Robustness to contrast sets'\n",
        "])\n",
        "def extract_scenario_between_colon_and_dot(response):\n",
        "    try:\n",
        "        start = response.index(\":\")\n",
        "        end = response.index(\".\")\n",
        "        scenario_name = response[start+2:end].strip()\n",
        "        # assert(scenario_name in SCENARIOS)\n",
        "    except:\n",
        "        return 'None'\n",
        "    return scenario_name\n",
        "\n",
        "def extract_first_between_brackets(text):\n",
        "    pattern = r\"(`|\\*\\*)(.*?)(`|\\*\\*)\"\n",
        "    matches = re.search(pattern, text)\n",
        "    if matches:\n",
        "        return matches.group(2)\n",
        "    else:\n",
        "        return 'None'\n",
        "\n",
        "def extract_first_number(text):\n",
        "    # Regular expression to match the first number (integer or float)\n",
        "    match = re.search(r\"[-+]?\\d*\\.\\d+|\\d+\", text)\n",
        "    if match:\n",
        "        return float(match.group())  # Convert the matched string to a float\n",
        "    else:\n",
        "        return None  # Return None if no number is found\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8eFeFQMq03Kv"
      },
      "source": [
        "## Call API function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "2SPcUlTMTcL_"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import requests\n",
        "import json\n",
        "def getResponseFromGemSUra(prompt):\n",
        "    start_time = time.time()\n",
        "    url = \"https://www.ura.hcmut.edu.vn/haystack/gemsura/api/generate_stream\"\n",
        "    payload = {\n",
        "      \"inputs\": f\"<start_of_turn>user\\n{prompt}<end_of_turn>\\n<start_of_turn>model\\n\",\n",
        "      \"parameters\": {\"temperature\": 0.1, \"max_new_tokens\": 8, \"return_full_text\": False},\n",
        "      \"options\": {\"use_cache\": False},\n",
        "      \"debug\": False\n",
        "    }\n",
        "    headers = {\n",
        "      \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "    response = requests.post(url, json=payload, headers=headers)\n",
        "    lines = response.text.split('\\n')\n",
        "    non_empty_lines = [line for line in lines if line.strip() != '']\n",
        "    output_string = '\\n'.join(non_empty_lines).replace(\"data:\",\"\")\n",
        "    lines = output_string.strip().split('\\n')\n",
        "    json_objects = [json.loads(line) for line in lines]\n",
        "    generated_text = json_objects[1][\"generated_text\"]\n",
        "    lines2 = generated_text.split('\\n')\n",
        "    non_empty_lines2 = [line for line in lines2 if line.strip() != '']\n",
        "    generated_text = '\\n'.join(non_empty_lines2)\n",
        "    processed_time = time.time() - start_time\n",
        "    return generated_text, processed_time\n",
        "\n",
        "def getResponseFromLLaMa70B(prompt):\n",
        "    start_time = time.time()\n",
        "    url = 'https://www.ura.hcmut.edu.vn/gemsura/api/generate_stream'\n",
        "    payload = {\n",
        "      \"inputs\": f\"<start_of_turn>user\\n{prompt}<end_of_turn>\\n<start_of_turn>model\\n\",\n",
        "      \"parameters\": {\"temperature\": 0.1, \"max_new_tokens\": 8, \"return_full_text\": False},\n",
        "      \"options\": {\"use_cache\": False},\n",
        "      \"debug\": False\n",
        "    }\n",
        "    headers = {\n",
        "      \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "    response = requests.post(url, json=payload, headers=headers)\n",
        "    lines = response.text.split('\\n')\n",
        "    non_empty_lines = [line for line in lines if line.strip() != '']\n",
        "    final_obj = json.loads(non_empty_lines[-1].replace(\"data:\", \"\"))\n",
        "    generated_text = final_obj[\"generated_text\"]\n",
        "    lines2 = generated_text.split('\\n')\n",
        "    non_empty_lines2 = [line for line in lines2 if line.strip() != '']\n",
        "    generated_text = '\\n'.join(non_empty_lines2)\n",
        "    processed_time = time.time() - start_time\n",
        "    return generated_text, processed_time\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YsT2gyuo060A"
      },
      "source": [
        "# Clone Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/Users/duc/Documents/Projects/orchestrator-of-llms-band\n"
          ]
        }
      ],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Df6RnJK49tfy",
        "outputId": "58109ae2-eebd-44e9-8cd2-0a7a55296198"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'truthful_qa'...\n",
            "remote: Enumerating objects: 33, done.\u001b[K\n",
            "remote: Counting objects: 100% (7/7), done.\u001b[K\n",
            "remote: Compressing objects: 100% (7/7), done.\u001b[K\n",
            "remote: Total 33 (delta 0), reused 0 (delta 0), pack-reused 26\u001b[K\n",
            "Unpacking objects: 100% (33/33), 12.74 KiB | 149.00 KiB/s, done.\n",
            "Filtering content: 100% (2/2), 482.11 KiB | 95.00 KiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "# !git clone https://huggingface.co/datasets/narrativeqa.git\n",
        "# !git clone https://huggingface.co/datasets/gsm8k.git\n",
        "# !git clone https://huggingface.co/datasets/truthful_qa.git\n",
        "# !git clone https://huggingface.co/datasets/stanfordnlp/imdb.git\n",
        "# !git clone https://huggingface.co/datasets/google/civil_comments\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDQPGm591DIV"
      },
      "source": [
        "## Test dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "fJyV8m4p-Cyh"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# parquet_file = 'narrativeqa/data/test-00000-of-00008.parquet' # NarativeQA  https://huggingface.co/datasets/narrativeqa.git    292/600     `Question-answering`\n",
        "# parquet_file = '0000.parquet' # BigBench https://huggingface.co/datasets/tasksource/bigbench.git     1/100     `Vary multiple-choice strategy`\n",
        "# parquet_file = 'gsm8k/main/test-00000-of-00001.parquet' # https://huggingface.co/datasets/gsm8k.git   2/150      `Reasoning`\n",
        "parquet_file = 'datasets/truthful_qa/generation/validation-00000-of-00001.parquet' # https://huggingface.co/datasets/truthful_qa.git  0/150       `Knowledge`\n",
        "# parquet_file = 'imdb/plain_text/test-00000-of-00001.parquet' # https://huggingface.co/datasets/stanfordnlp/imdb.git    0/150   `Sentiment analysis`\n",
        "# parquet_file = 'civil_comments/data/test-00000-of-00001.parquet' # https://huggingface.co/datasets/google/civil_comments    5/100   `Toxicity detection`\n",
        "# parquet_file = '' # https://huggingface.co/datasets/EdinburghNLP/xsum.git # !tar -zxvf xsum/data/XSUM-EMNLP18-Summary-Data-Original.tar.gz\n",
        "# parquet_file = '0000TwitterAAE.parquet' # https://huggingface.co/datasets/lighteval/TwitterAAE/blob/refs%2Fconvert%2Fparquet/aa/test/0000.parquet     `Language`\n",
        "# parquet_file = '0000Xsum.parquet' # https://huggingface.co/datasets/EdinburghNLP/xsum/tree/refs%2Fconvert%2Fparquet/default/test     `Summarization`\n",
        "\n",
        "df = pd.read_parquet(parquet_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QUB5sSyj_C4e",
        "outputId": "84b97493-2e32-4b64-a46c-663ea79669a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "type                                                       Adversarial\n",
            "category                                                Misconceptions\n",
            "question              What happens to you if you eat watermelon seeds?\n",
            "best_answer          The watermelon seeds pass through your digesti...\n",
            "correct_answers      [Nothing happens, You eat watermelon seeds, Th...\n",
            "incorrect_answers    [You grow watermelons in your stomach, You get...\n",
            "source               https://wonderopolis.org/wonder/will-a-waterme...\n",
            "Name: 0, dtype: object\n"
          ]
        }
      ],
      "source": [
        "print(df.iloc[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "dqt35CDS9iOJ",
        "outputId": "3ff27751-2c8f-4675-9957-d2310351123a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'According to the Bible, what forbidden fruit did Adam and Eve eat in the Garden of Eden?'"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.iloc[14]['question']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RA1oesT1HSq"
      },
      "source": [
        "## Create PROMPT set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "3iIPEBYm_VQ-",
        "outputId": "2f349a13-966f-4091-9c98-daab7006d1d2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"What is the world's favourite airline?\""
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_sub = df[100:300]\n",
        "# PROMPTS = [df_sub.iloc[i]['question']['text'] for i in range(len(df_sub))]\n",
        "# PROMPTS = [df_sub.iloc[i]['inputs'] + ' ' + '\\n'.join(df_sub.iloc[i]['multiple_choice_targets']) for i in range(len(df_sub))]\n",
        "# PROMPTS = [df_sub.iloc[i]['text'] for i in range(len(df_sub))]\n",
        "# PROMPTS = [df_sub.iloc[i]['question'] for i in range(len(df_sub))]\n",
        "PROMPTS = [df_sub.iloc[i]['question'] for i in range(len(df_sub))]\n",
        "\n",
        "PROMPTS[50]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_S9Fl1F1LWV"
      },
      "source": [
        "# Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "9H44iRx1bWtb"
      },
      "outputs": [],
      "source": [
        "results = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SMgssf8V4uAF",
        "outputId": "d81c3f2f-a316-4067-af3f-8a4430f3d3f5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from IPython.display import display, HTML\n",
        "import requests\n",
        "import json\n",
        "\n",
        "i = 0\n",
        "\n",
        "for prompt in PROMPTS[:]:\n",
        "    i += 1\n",
        "    result = {}\n",
        "    cost = 0\n",
        "\n",
        "    print(f'{i}: {prompt[:150]}...')\n",
        "    for scenario, inner_info in SCENARIOS_INFO.items():\n",
        "        fprompt = format_prompt(\n",
        "            prompt,\n",
        "            scenario,\n",
        "            inner_info['details']\n",
        "        )\n",
        "\n",
        "        while True:\n",
        "            try:\n",
        "                response, processed_time = getResponseFromLLaMa70B(fprompt)\n",
        "            except:\n",
        "                continue\n",
        "            break\n",
        "\n",
        "        print(f'\\t{response[:150]}...')\n",
        "        score = extract_first_number(response)\n",
        "\n",
        "        result[scenario] = score\n",
        "        cost += processed_time\n",
        "        print(f'\\t\\t{scenario}: {score}')\n",
        "\n",
        "    print(f'{cost:.2f}s')\n",
        "    results.append((result, cost))\n",
        "    \n",
        "    with open('results_Knowledge_2.json', 'w') as json_file:\n",
        "        json.dump(results, json_file, indent=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOVFHtGV1OK_"
      },
      "source": [
        "## Analyze"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2GvL10JZNlU"
      },
      "source": [
        "Clean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "UaS2VWooRAOW"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "37 Information retrieval\n",
            "55 Language\n",
            "65 Question-answering\n",
            "73 Information retrieval\n"
          ]
        }
      ],
      "source": [
        "for i, (result, t) in enumerate(results):\n",
        "    for task, score in result.items():\n",
        "        if not score or score < 1 or score > 5:\n",
        "            print(i, task)\n",
        "            result[task] = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcMRSTAOaXac"
      },
      "source": [
        "Softmax -> Find max"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "en2B3VVnZsJu"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def softmax(input_dict):\n",
        "    values = np.array(list(input_dict.values()))\n",
        "    exp_values = np.exp(values - np.max(values))  # Subtracting max value for numerical stability\n",
        "    softmax_values = exp_values / np.sum(exp_values)\n",
        "\n",
        "    softmax_dict = {key: softmax_values[i] for i, key in enumerate(input_dict.keys())}\n",
        "\n",
        "    return softmax_dict\n",
        "\n",
        "def max_keys(softmax_dict):\n",
        "    max_value = max(softmax_dict.values())\n",
        "    return [key for key, value in softmax_dict.items() if value == max_value]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "Md1ZXuK4ZQuu"
      },
      "outputs": [],
      "source": [
        "normalized_results = []\n",
        "maxes_list = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "KuCOYykgZOw8"
      },
      "outputs": [],
      "source": [
        "for result, t in results:\n",
        "    softmax_result = softmax(result)\n",
        "    normalized_results.append(softmax_result)\n",
        "    maxes_list.append(max_keys(softmax_result))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7q5raWjb2Wf"
      },
      "source": [
        "Count if max is the target scenario"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "Y52IOQgFcbWd"
      },
      "outputs": [],
      "source": [
        "TARGET_SCENARIO = 'Summarization'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "YfB95kKraWV6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "60"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "count = 0\n",
        "\n",
        "for maxes in maxes_list:\n",
        "    if TARGET_SCENARIO in maxes:\n",
        "        count += 1\n",
        "\n",
        "count"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
